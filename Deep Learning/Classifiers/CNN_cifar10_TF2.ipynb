{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "CNN_cifar10_TF2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQBoYvdUKYfz"
      },
      "source": [
        "# CNN with Cifar10\n",
        "\n",
        "Example of Convolutional Neural Net (CNN) with TensorFlow (TF) 2.0.\n",
        "\n",
        "This example builds a CNN from scratch, with custom layers and training.\n",
        "\n",
        "**Sources:**\n",
        "\n",
        "* [Udemey_tf_course](https://www.udemy.com/course/complete-guide-to-tensorflow-for-deep-learning-with-python/)\n",
        "* [TF_tutorial](https://www.tensorflow.org/tutorials/customization/custom_training)\n",
        "* [Medium_blog](https://becominghuman.ai/image-classification-with-tensorflow-2-0-without-keras-e6534adddab2)\n",
        "\n",
        "`Author: Rodrigo Vimieiro`\n",
        "\n",
        "`Date: Apr, 2020`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7fXEXhZdqF"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/LAVI-USP/Machine-Learning/blob/master/Deep%20Learning/Classifiers/CNN_cifar10_TF2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/LAVI-USP/Machine-Learning/blob/master/Deep%20Learning/Classifiers/CNN_cifar10_TF2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIyaCPYoSM1w"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTFiZhZKRNbj"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMSUK_MMIVQ"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xnTpXBRNbr"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzK_0vrJRNbx"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhL1ourPRNbz"
      },
      "source": [
        "# Source: https://www.udemy.com/course/complete-guide-to-tensorflow-for-deep-learning-with-python/\n",
        "class CifarHelper():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.i = 0\n",
        "        \n",
        "        # Intialize some empty variables for later on\n",
        "        self.training_images = None\n",
        "        self.training_labels = None\n",
        "        \n",
        "        self.test_images = None\n",
        "        self.test_labels = None\n",
        "    \n",
        "    def set_up_images(self):\n",
        "        \n",
        "        print(\"Setting Up Training Images and Labels\")\n",
        "        \n",
        "        self.training_images = train_images / 255.0\n",
        "        self.training_labels = self.one_hot_encode(train_labels) \n",
        "        \n",
        "        print(\"Setting Up Test Images and Labels\")\n",
        "        \n",
        "        self.test_images = test_images / 255.0\n",
        "        self.test_labels = self.one_hot_encode(test_labels)\n",
        "\n",
        "    def one_hot_encode(self, vec):\n",
        "\n",
        "        n = len(vec)\n",
        "        out = np.zeros((n, 10))\n",
        "        for i in range(n):\n",
        "            out[i, vec[i]] = 1\n",
        "        \n",
        "        return out\n",
        "\n",
        "        \n",
        "    def next_batch(self, batch_size):\n",
        "        \n",
        "        x = self.training_images[self.i:self.i+batch_size]\n",
        "        y = self.training_labels[self.i:self.i+batch_size]\n",
        "        self.i = (self.i + batch_size) % len(self.training_images)\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea4eE3TpRNb4"
      },
      "source": [
        "ch = CifarHelper()\n",
        "ch.set_up_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BImiokQiRNcA"
      },
      "source": [
        "## Layer functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZVrA1d1RNcM"
      },
      "source": [
        "# Convolutional layer\n",
        "def conv_layer(input_x,w,b):\n",
        "  \n",
        "  # input_x -> [batch,H,W,Channels]\n",
        "  # filter_shape -> [filters H, filters W, Channels In, Channels Out]\n",
        "\n",
        "  y = tf.nn.conv2d(input=input_x,filters=w,strides=[1,1,1,1],padding='SAME') + b\n",
        "\n",
        "  y = tf.nn.relu(y)\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G3stwp9RNcP"
      },
      "source": [
        "# Pooling layer\n",
        "def maxPool_layer(x,poolSize):\n",
        "  # x -> [batch,H,W,Channels]\n",
        "\n",
        "  return tf.nn.max_pool2d(input=x,ksize=[1,poolSize,poolSize,1],strides=[1,poolSize,poolSize,1],padding=\"SAME\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSQ1MnmoRNcS"
      },
      "source": [
        "# Fully connected layer\n",
        "def fullyConnected_layer(input_layer,w,b):\n",
        "\n",
        "  y = tf.matmul(input_layer,w) + b\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2qte6qwRNcU"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYhnebC5RNcV"
      },
      "source": [
        "def get_tfVariable(shape, name):\n",
        "\n",
        "  return tf.Variable(tf.random.truncated_normal(shape,stddev=0.1), name=name, trainable=True, dtype=tf.float32)\n",
        "\n",
        "class my_model():\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    self.pool_size = 2\n",
        "    self.dropout = 0.5\n",
        "    self.nclasses = 10\n",
        "\n",
        "    self.shapes = [\n",
        "    [5, 5, 3, 32], \n",
        "    [5, 5, 32, 64],\n",
        "    [8*8*64,512],\n",
        "    [512, self.nclasses]\n",
        "    ]\n",
        "\n",
        "    self.weights = []\n",
        "    for i in range(len(self.shapes)):\n",
        "      self.weights.append( get_tfVariable(self.shapes[i] , 'weight{}'.format( i ) ) )\n",
        "\n",
        "    self.bias = []\n",
        "    for i in range(len(self.shapes)):\n",
        "      self.bias.append( get_tfVariable([1,self.shapes[i][-1]] , 'bias{}'.format( i ) ) )\n",
        "\n",
        "\n",
        "\n",
        "  def run(self, x_input):\n",
        "    \n",
        "    conv1 = conv_layer(x_input,self.weights[0],self.bias[0]) \n",
        "    pool1 = maxPool_layer(conv1,poolSize=self.pool_size)\n",
        "    \n",
        "    conv2 = conv_layer(pool1,self.weights[1],self.bias[1]) \n",
        "    pool2 = maxPool_layer(conv2,poolSize=self.pool_size)\n",
        "    \n",
        "    flat1 = tf.reshape(pool2,[-1,pool2.shape[1]*pool2.shape[2]*pool2.shape[3]])\n",
        "    \n",
        "    fully1 = tf.nn.relu(fullyConnected_layer(flat1,self.weights[2],self.bias[2]))\n",
        "    \n",
        "    fully1_dropout = tf.nn.dropout(fully1,rate=self.dropout)\n",
        "    \n",
        "    y_pred = fullyConnected_layer(fully1_dropout,self.weights[3],self.bias[3])\n",
        "    \n",
        "    #print(conv1.shape,pool1.shape,conv2.shape,pool2.shape,flat1.shape,fully1.shape,y_pred.shape)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "  def trainable_variables(self):\n",
        "\n",
        "    return self.weights + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7WbbkKtqqPB"
      },
      "source": [
        "model = my_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TApZkgNSRNcZ"
      },
      "source": [
        "## Creating loss function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxbfCHeBRNca"
      },
      "source": [
        "def loss_function(y_pred,y_true):\n",
        "    \n",
        "    return tf.nn.softmax_cross_entropy_with_logits(labels=tf.stop_gradient(y_true),logits=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0safi1Byvba"
      },
      "source": [
        "## Creating optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV6VfxWARNcd"
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5TpXTcHRNcf"
      },
      "source": [
        "## Trainning function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVqWc5OWRNcg"
      },
      "source": [
        "def train_step( model, x_input , y_true, epoch):\n",
        "\n",
        "  epoch_accuracy = None\n",
        "  epoch_loss_avg = None\n",
        "    \n",
        "  with tf.GradientTape() as tape:\n",
        "        \n",
        "    # Get the predictions\n",
        "    preds = model.run(x_input)\n",
        "        \n",
        "    # Calc the loss\n",
        "    current_loss = loss_function(preds,y_true)\n",
        "    \n",
        "    # Get the gradients\n",
        "    grads = tape.gradient(current_loss, model.trainable_variables())\n",
        "    \n",
        "    # Update the weights\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables()))\n",
        "    \n",
        "    if epoch%100 == 0:\n",
        "\n",
        "      y_pred = model.run(ch.test_images)\n",
        "      matches  = tf.equal(tf.math.argmax(y_pred,1), tf.math.argmax(ch.test_labels,1))\n",
        "\n",
        "      epoch_accuracy = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
        "      epoch_loss_avg = tf.reduce_mean(current_loss)\n",
        "\n",
        "      print(\"--- On epoch {} ---\".format(epoch))\n",
        "      tf.print(\"Accuracy: \", epoch_accuracy, \"| Loss: \",epoch_loss_avg)\n",
        "      print(\"\\n\")\n",
        "\n",
        "    return epoch_accuracy,epoch_loss_avg\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdJPJmruy8yY"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6_c6ZqDRNci"
      },
      "source": [
        "num_epochs = 5000\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "  # Get next batch\n",
        "  batch_x, batch_y = ch.next_batch(batch_size)\n",
        "    \n",
        "  # Train the model\n",
        "  epoch_accuracy, epoch_loss_avg = train_step(model, batch_x, batch_y, epoch)\n",
        "\n",
        "  if(epoch_loss_avg is not None):\n",
        "    train_loss_results.append(epoch_loss_avg)\n",
        "    train_accuracy_results.append(epoch_accuracy)\n",
        "\n",
        "plt.plot(train_loss_results)\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "plt.title('Accuracy')\n",
        "plt.plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Islp_Bo1oLCj"
      },
      "source": [
        "n = 784\n",
        "pred = model.run(ch.test_images[n:n+1])\n",
        "tf.print(tf.math.argmax(pred,1))\n",
        "tf.print(tf.math.argmax(ch.test_labels[n:n+1],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbqUTsSZ8alC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}